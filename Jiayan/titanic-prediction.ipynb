{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nimport sklearn\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        filename = os.path.join(dirname, filename)\n        if \"train\" in filename:\n            X_train_origin = pd.read_csv(filename)\n        elif 'test' in filename:\n            X_test_origin = pd.read_csv(filename)\n        else:\n            submission = pd.read_csv(filename)\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_origin.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_origin.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_origin.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data visualization\nX_train_origin.describe()\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_origin.select_dtypes(include = ['object']).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create xtrain, xtest, ytrain\ny_train = X_train_origin['Survived']\nx_test_id = X_test_origin['PassengerId']\nX_train = X_train_origin.drop(['PassengerId','Survived', 'Name', 'Ticket', 'Cabin'], axis = 1)\nX_test = X_test_origin.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preprocessing the data\nall_X = pd.concat([X_train, X_test], keys = ['train', 'test'], axis = 0)\n#dealing with NaN\nvariables = list(all_X.columns)\nfor v in variables:\n    print(v, all_X[v].isnull().sum())\nall_X.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_X.select_dtypes(include = ['object']).describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill age with mean, Cabin with \"0\", embarked with S, Fare with median\nX_train['Age'].fillna(30, inplace = True)\nX_test['Age'].fillna(30, inplace = True)\n\n# X_train['Cabin'].fillna('0', inplace = True)\n# X_test['Cabin'].fillna('0', inplace = True)\n\nX_train['Embarked'].fillna('S', inplace = True)\nX_test['Embarked'].fillna('S', inplace = True)\n\nX_train['Fare'].fillna(14.45, inplace = True)\nX_test['Fare'].fillna(14.45, inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.isnull().sum().sum(), X_test.isnull().sum().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding the variables\ncategorical = ['Sex','Embarked']\nfor c in categorical:\n    enc = preprocessing.OneHotEncoder(handle_unknown='ignore')\n    enc_df = pd.DataFrame(enc.fit_transform(X_train[[c]]).toarray())\n    for j in list(enc_df.columns):\n        X_train[c + \"_\" + str(j)] = enc_df[j]\n    enc_df = pd.DataFrame(enc.fit_transform(X_test[[c]]).toarray())\n    for j in list(enc_df.columns):\n        X_test[c + \"_\" + str(j)] = enc_df[j]\n    X_train = X_train.drop([c], axis = 1)\n    X_test = X_test.drop([c], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalzied the dataset\nsc_X = preprocessing.StandardScaler()\nX_train_standarlized = sc_X.fit_transform(X_train)\nX_test_standarlized = sc_X.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#feature selection\n# features = list(X_train.columns)\n# from sklearn.ensemble import RandomForestClassifier\n# RF = RandomForestClassifier(n_estimators = 100, bootstrap = False)\n# RF.fit(X_train, y_train)\n# f_rf = {}\n# for i in range(len(features)):\n#     f_rf[features[i]] = list(RF.feature_importances_)[i]\n# f_rf = sorted(f_rf.items(), key = lambda x : x[1], reverse = True)\n# print(f_rf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model selection\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.model_selection import StratifiedKFold\nfrom statistics import mean, stdev\n#stratified cross validation\ndef cross_validation(X, Y, clf, name, K = 5):\n    x = X.values\n    y = Y.values\n    skf = StratifiedKFold(n_splits = K, shuffle = False)\n    lst_accu_stratified = []\n    for train_index, test_index in skf.split(x,y):\n        x_train_fold, x_test_fold = x[train_index], x[test_index] \n        y_train_fold, y_test_fold = y[train_index], y[test_index] \n        clf.fit(x_train_fold, y_train_fold) \n        lst_accu_stratified.append(clf.score(x_test_fold, y_test_fold))\n    print(name)\n    print('\\nMaximum Accuracy That can be obtained from this model is:', \n          max(lst_accu_stratified)*100, '%') \n    print('\\nMinimum Accuracy:', \n          min(lst_accu_stratified)*100, '%') \n    print('\\nOverall Accuracy:', \n          mean(lst_accu_stratified)*100, '%') \n    print('\\nStandard Deviation is:', stdev(lst_accu_stratified))\n    print(\"\")\n    \n#Logistic regression\ndef LR(X_train, y_train):\n    clf = LogisticRegression(random_state = 10)\n    cross_validation(X_train, y_train, clf, 'LR')\n\n#support vector machine\ndef SVM(X_train, y_train):\n    clf = svm.SVC(probability = True,kernel = 'rbf')\n    cross_validation(X_train, y_train, clf, 'SVM')\n\n#RandomForest\ndef RF(X_train, y_train):\n    clf = RandomForestClassifier(n_estimators = 100, bootstrap = False,random_state = 10)\n    cross_validation(X_train, y_train, clf, 'RF')\n\n#Decision tree\ndef DT(X_train, y_train):\n    clf = DecisionTreeClassifier(random_state = 10)\n    cross_validation(X_train, y_train, clf, 'DT')\n\n#KNN\ndef KNN(X_train, y_train):\n    clf = KNeighborsClassifier(n_neighbors = int(len(X_train)**0.5), p = 2, metric = 'euclidean')\n    cross_validation(X_train, y_train, clf, 'KNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR(X_train, y_train)\nRF(X_train, y_train)\nSVM(X_train, y_train)\nDT(X_train, y_train)\nKNN(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n#grid search for optimization\nn_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\nclf = RandomForestClassifier(random_state = 1)\n\ngridF = GridSearchCV(clf, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction\nclf = RandomForestClassifier(n_estimators = 100, min_samples_split = 10, min_samples_leaf = 2, max_depth = 15, bootstrap = False,random_state = 10)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission file\nsubmission = pd.DataFrame({'PassengerId': x_test_id,\n                           'Survived': predictions})\nsubmission.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(gridF.best_params_) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}